} else{
# compute number of tables serving the current dish in the whole franchise, excluding future individuals
nTablesServingCurrentDishMH = length(table(observationTableAllocationMH[observationDishAllocationMH == currentDish]))
# get the tables in current restaurant serving the current dish (excluding future observations)
# tablesInRestaurantServingCurrentDish = (1:maxTableIndexMH)[((tableRestaurantAllocationMH == newRestaurantAllocation)&(tablesValuesMH == currentDish))]
logP_MH = logP_MH +
log(nTablesServingCurrentDishMH - sigma0) -
log(theta0 + nTablesMH)
}
### update quantities of MH
# nPeopleInRestaurantMH = nPeopleInRestaurantMH + 1
dishesCountsMH[currentDish] = dishesCountsMH[currentDish] + 1
observationDishAllocationMH[indexCustomerGlobalMH] = currentDish
nTablesMH = nTablesMH + 1
if(nFreeTablesMH > 0) { # pick the first free table
newTableAllocation = freeTablesMH[1]
freeTablesMH = freeTablesMH[-1]
nFreeTablesMH = nFreeTablesMH - 1
nPeopleAtTableMH[newTableAllocation] = 1
tablesValuesMH[newTableAllocation] = observationDishAllocationMH[indexCustomerGlobalMH]
tableRestaurantAllocationMH[newTableAllocation] = newRestaurantAllocation # assign table to restaurant
} else { # create a new table
maxTableIndexMH = maxTableIndexMH + 1
newTableAllocationMH = maxTableIndexMH
nPeopleAtTableMH = c(nPeopleAtTableMH,1)
tablesValuesMH = c(tablesValuesMH,observationDishAllocationMH[indexCustomerGlobalMH])
tableRestaurantAllocationMH = c(tableRestaurantAllocationMH,newRestaurantAllocation) # assign table to restaurant
}
} else{ # the sampled table is already occupied in the restaurant --> just update the relevant quantities
nPeopleAtTableMH[newTableAllocation] = nPeopleAtTableMH[newTableAllocation] + 1
}
observationTableAllocationMH[indexCustomerGlobalMH] = newTableAllocation
# update indexCustomerGlobalMH
indexCustomerGlobalMH = indexCustomerGlobalMH + 1
}
pAccept = min(exp(logP_MH - logP_CS),1)
accept = runif(1) < pAccept
# print(pAccept)
if(accept == T){
nRest = nRestMH
maxRestIndex = maxRestIndexMH
groupRestaurantAllocation[indexGroup] = groupRestaurantAllocationMH[indexGroup]
nGroupsInRestaurant = nGroupsInRestaurantMH
nFreeRestaurants = nFreeRestaurantsMH
freeRestaurants = freeRestaurantsMH
maxTableIndex = maxTableIndexMH
observationTableAllocation = observationTableAllocationMH
observationRestaurantAllocation = observationRestaurantAllocationMH
nPeopleAtTable = nPeopleAtTableMH
nTables = nTablesMH
freeTables = freeTablesMH
nFreeTables = nFreeTablesMH
tablesValues = tablesValuesMH
tableRestaurantAllocation = tableRestaurantAllocationMH
}
##### END OF METROPOLIS HASTINGS STEP
### EDIT THIS PART TO UPDATE THE TABLES ALLOCATIONS FOR THE WHOLE GROUP
groupRestaurantAllocationAcrossGibbs[r, indexGroup] = groupRestaurantAllocation[indexGroup]
observationTableAllocationAcrossGibbs[r,firstIndividuals[indexGroup]:lastIndividuals[indexGroup]] =
observationTableAllocation[firstIndividuals[indexGroup]:lastIndividuals[indexGroup]]
}
### SAMPLE TABLE AND DISH FOR OUT-OF-SAMPLE OBSERVATIONS
# THIS HAS BEEN MOVED OUT OF THE LOOP FOR THE MOMENT
}
nTablesServingCurrentDish
sigma0
log(nTablesServingCurrentDishCS - sigma0)
nTablesServingCurrentDishCS
g=0.1; p=0.9; (1+g)/p + (1-p)/p
g=0.001; p=0.999; (1+g)/p + (1-p)/p
60/16
60/76
10/11
10000/(40*60)
unlink("Library/CloudStorage/Dropbox/Teaching/UNITO/ISL/Rebaudo/Rebaudo - 2023:24/Lect 13-14 Cross Validation and Resampling upload/Chp 5 - Validation and Resampling_cache", recursive = TRUE)
library(ISLR2)
# Lab: Cross-Validation and the Bootstrap
library(ISLR2)
?library
set.seed(1)
?Auto
train <- sample(392,196)
train
lm.fit <- lm(mpg~horsepower, data=Auto, subset = train)
lm.fit
summary(lm.fit)
attach(Auto)
mpg
mean( (mpg-predict(lm.fit, Auto))[-train]^2 )
plot(mpg~horsepower)
lm.fit2 <- lm(mpg~poly(horsepower,2), data=Auto, subset = train)
mean( (mpg-predict(lm.fit2, Auto))[-train]^2 )
lm.fit3 <- lm(mpg~poly(horsepower,3), data=Auto, subset = train)
mean( (mpg-predict(lm.fit3, Auto))[-train]^2 )
set.seed(2)
train <- sample(392,196)
set.seed(2)
train <- sample(392,196)
lm.fit <- lm(mpg~horsepower, data=Auto, subset = train)
mean( (mpg-predict(lm.fit, Auto))[-train]^2 )
lm.fit2 <- lm(mpg~poly(horsepower,2), data=Auto, subset = train)
mean( (mpg-predict(lm.fit2, Auto))[-train]^2 )
lm.fit3 <- lm(mpg~poly(horsepower,3), data=Auto, subset = train)
mean( (mpg-predict(lm.fit3, Auto))[-train]^2 )
glm.fit(mpg~horsepower, data=Auto)
glm.fit(mpg~horsepower)
glm.fit <- glm(mpg~horsepower, data=Auto)
coef(glm.fit)
glm.fit <- glm(mpg~horsepower, data=Auto)
coef(glm.fit)
lm.fit <- lm(mpg~horsepower, data=Auto)
coef(lm.fit)
library(boot)
library(boot)
cv.err <- cv.glm(Auto, glm.fit)
cv.err
cv.err$delta
cv.error <- double(10)
cv.error
cv.error <- double(10)
for (i in 1:10){
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
plot(cv.error,type="b")
loocv <- function(fit){
h <- lm.influence(fit)$h
mean((residual(fit)/(1-h))^2)
}
set.seed(17)
set.seed(17)
cv.error.10 <- rep(0, 10)
cv.error.10
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10){
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.error[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10){
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
lines(cv.error.10, type="b", col=2)
i
i=1
glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
cv.glm(Auto, glm.fit, K=10)$delta
cv.glm(Auto, glm.fit)$delta
alpha.fn <- function(data, index){
X <- data$X[index]
Y <- data$Y[index]
(var(Y)-cov(X,Y))/(Var(X)+Var(Y)-2*cov(X,Y))
}
alpha.fn(Portfolio,1:100)
alpha.fn <- function(data, index){
X <- data$X[index]
Y <- data$Y[index]
(var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))
}
alpha.fn(Portfolio,1:100)
alpha.fn(Portfolio, sample(100, 100, replace=T))
boot(Portfolio, alpha.fn, R=1000)
alpha.fn(Portfolio,1:100)
boot.fn <- function(data, index){}
boot.fn <- function(data, index){
coef(lm(mpg~horsepower, data=data, subset=index))
}
boot.fn(Auto, 1:392)
set.seed(1)
boot.fn(Auto, sample(392, 392, replace=T))
boot(Auto, boot.fn, 1000)
summary(lm(mpg~horsepower, data=Auto))$coef
94/4
23.5/4
library(ISLR2)
x <- model.matrix(Salary ~ ., Hitters)
x
x <- model.matrix(Salary ~ ., Hitters)[,-1]
x
y <- Hitters$Salary
y
Hitters <- na.omit(Hitters)
library(ISLR2)
library(leaps)
Hitters <- na.omit(Hitters)
regfit.bwd <- regsubsets(Salary ~., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
plot(regfit.bwd, scale = "cp")
plot(regfit.bwd, scale = "Cp")
library(glmnet)
x <- model.matrix(Salary~., Hitters)
x
x <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
grid <- 10^seq(10,-2, length=100)
grid
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
plot(ridge.mod, xvar = "lambda", label = TRUE)
ridge.mod$lambda[50]
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50])^2)
sqrt(sum(coef(ridge.mod)[-1,100])^2)
ridge.mod$lambda[100]
predict(ridge.mod, s=50, type="coefficient")[1:20,]
set.seed(1)
cv.out <- cv.glmnet(x, y, alpha=0)
plot(cv.out)
set.seed(1)
train <- sample(1:nrow(x), nrow/2)
train <- sample(1:nrow(x), nrow(x)/2)
train
test <- (-train)
test
train
y.test <- y[test]
y.test
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda = grid, thresh=1e-12)
ridge.mod <- predict(ridge.mod, s=4, newx = x[test,])
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda = grid, thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx = x[test,])
ridge.pred
mean((ridge.pred-y.test)^2)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
cv.out$lambda.min
bestlam <- cv.out$lambda.min
ridge.pred <- predict(ridge.mod, s=bestlam, newx = x[test,])
mean((ridge.pred-y.test)^2)
# Lasso
lasso.mod <- glmnet(x[train,], y[train], alpha=1, lambda = grid)
plot(lasso.mod)
plot(lasso.mod, xvar="lambda")
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
ridge.mod <- glmnet(x[train,], y[train], alpha=0, lambda = grid, thresh=1e-12)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
bestlam <- cv.out$lambda.min
ridge.pred <- predict(ridge.mod, s=bestlam, newx = x[test,])
mean((ridge.pred-y.test)^2)
mean((lasso.pred-y.test)^2)
mean((ridge.pred-y.test)^2)
# PCR
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary~., data=Hitters, scale=TRUE, validationplot="CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type="MSEP")
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train, scale=T, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
library(updateR)
updateR()
2+2
library(ISLR2)
attach(Wage)
fit <- lm(wage ~ poly(age, 4), data=Wage)
coef(summary(fit))
fit2 <- lm(wage ~ poly(age, 4, raw=T), data=Wage)
coef(summary(fit2))
fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data=Wage)
coef(summary(fit2a))
agelims <- range(age)
age.grid <- seq(from=agelims[1], to=agelims[2])
preds <- predict(fit, newdata = list(age=age.grid), se=T)
se.bands <- cbind(preds$fit + 2*preds$se.fit,  preds$fit - 2*preds$se.fit)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)
preds2 <- predict(fit2, newdata=list(age=age.grid), se=T)
plot(preds$fit, preds2$fit)
fit.1 <- lm(wage ~ age, data=Wage)
fit.2 <- lm(wage ~ poly(age,2), data=Wage)
fit.1 <- lm(wage ~ age, data=Wage)
fit.2 <- lm(wage ~ poly(age,2), data=Wage)
fit.3 <- lm(wage ~ poly(age,3), data=Wage)
fit.4 <- lm(wage ~ poly(age,4), data=Wage)
fit.5 <- lm(wage ~ poly(age,5), data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
fit <- glm(I(wage > 250) ~ poly(age, 4), data=Wage, family = binomial)
preds <-  predict(fit, newdata=list(age=age.grid), se=T)
preds
exp(preds$fit)/(1 + exp(preds$fit))
pfit <- exp(preds$fit)/(1 + exp(preds$fit))
pfit
se.bands.logit <- cbind(preds$fit + 2 *preds$se.fit, preds$fit - 2 * preds$se.fit)
se.bands <-  exp(se.bands.logit)/(1 + exp(se.bands.logit))
plot(age, I(wage > 250), xlim=agelims, type="n", ylim=c(0, 0.2))
points(jitter(age), I(wage > 250)/5, cex=.5, pch="|", col="darkgrey")
lines(age.grid, pfit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)
fit <- lm(wage ~ cut(age,4), data=Wage)
table(cut(age,4))
coef(fit)
library(splines)
fit <-lm(wage~ bs(age, knots=c(25,40,60)), data=Wage)
pred <- predict(fit, newdata=list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred$fit, lwd=2, col="blue")
lines(age.grid, pred$fit + 2 * pred$se , col="blue", lty="dashed")
lines(age.grid, pred$fit - 2 * pred$se , col="blue", lty="dashed")
bs(age, df=6, "knots")
attr(bs(age, df=6), "knots")
fit2 <- lm(wage~ns(age, df=4),)
fit2 <- lm(wage~ns(age, df=4),data=Wage)
pred2 <- predict(fit2, newdata = list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred$fit2, lwd=2, col="blue")
fit2 <- lm(wage~ns(age, df=4),data=Wage)
pred2 <- predict(fit2, newdata = list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred2$fit2, lwd=2, col="blue")
lines(age.grid, pred2$fit, lwd=2, col="blue")
fit2 <- lm(wage~ns(age, df=4),data=Wage)
pred2 <- predict(fit2, newdata = list(age=age.grid), se=T)
plot(age, wage, col="grey")
lines(age.grid, pred2$fit, lwd=2, col="blue")
plot(age, wage, col="grey")
lines(age.grid, pred$fit, lwd=2, col="blue")
fit <- smooth.spline(age, wage, df=16)
fit2 <- smooth.spline(age, wage, cv=T)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
lines(fit, col="red", lwd=2)
lines(fit2, col="blue", lwd=2)
legend("topright", legend("16 DF", "6.8 DF"), col=c("red", "blue"), lty=1, lwd=2, cex=.8)
legend("topright", legend("16 DF", "6.8 DF"), col=c("red", "blue"), lty=1, lwd=2, cex=.8)
legend("topright", legend=c("16 DF", "6.8 DF"), col=c("red", "blue"),
lty=1, lwd=2, cex=.8)
gam1 <- lm(wage ~ ns(year, 4) + ns(age,5) + education, data=Wage)
library(gam)
gam2 <- gam(wage ~ s(year,4) + s(age, 5) +education, data=Wage)
par(mfrow=c(1,3))
plot(gam2, se=T, col="blue")
# Load relevant libraries, functions and data ----------------------------------
rm(list=ls())
# Set the working directory to the current folder
# Code to set the working directory to the current folder from RStudio
library(rstudioapi) # version 0.15
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr)
library(tidyr)
library(mvtnorm)
library(invgamma)
library(splines)
library(ggplot2)
theme_set(theme_bw(base_size = 14))
library(reshape2)
set.seed(1992)
source("SEP_fcts.R")
## global variables for the dta
out = readDta_reg(file="/Data-and-Results/data_protein.RData")
## global variables for the dta
out = readDta_reg(file="Data-and-Results/data_protein.RData")
## make global vars for the data
X = out$X
y = out$y
my = mean(y,na.rm=T)
sy =  mean(apply(y,2,var,na.rm=T))
ages = out$ages ## age grid (for plotting)
C=out$C
warnings()
# Load relevant libraries, functions and data ----------------------------------
rm(list=ls())
# Set the working directory to the current folder
# Code to set the working directory to the current folder from RStudio
library(rstudioapi) # version 0.15
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr)
library(tidyr)
library(mvtnorm)
library(invgamma)
library(splines)
library(ggplot2)
theme_set(theme_bw(base_size = 14))
library(reshape2)
set.seed(1992)
source("SEP_fcts.R")
## global variables for the dta
out = readDta_reg(file="Data-and-Results/data_protein.RData")
## make global vars for the data
X = out$X
y = out$y
my = mean(y,na.rm=T)
sy =  mean(apply(y,2,var,na.rm=T))
ages = out$ages ## age grid (for plotting)
C=out$C
R=out$R
p=out$p
## prior pars
prior = list(
## column effects (proteins)
meta = rep(0,p),
Seta = diag(p),  ## might want to reduce var of trt offsets (2nd part)
aeta = 1,        ## eta[j] ~ G_eta; G_eta ~ PY(aeta, N(meta,Seta))
beta = 0.05, #- discount parameter
## row effects (patients)
mxi = my, ## xi[i] ~ G_xi; G_xi ~ PY(axi, N(mxi,Sxi))
Sxi = 25, # large to favor assignment of common effect to patients instead of protein
axi = 0.1,
bxi = -0.1, # # 0 as a DP
## hyperpars theta=sig2
asig = 2,
bsig = 2) ## w=1/sigs ~ Ga(asig/2, bsig/2); E(w) = m=a/b, V(w)=m/(b/2)
# Run MCMC
if (FALSE){
startTime = Sys.time()
main_reg(6000)
timeREG = difftime(Sys.time(), startTime, units=("secs"))[[1]]
}
# Codes accompanying "Separate Exchangeability as
# Modeling Principle in Bayesian Nonparametrics"
# Load relevant libraries, functions and data ----------------------------------
rm(list=ls())
# Set the working directory to the current folder
# Code to set the working directory to the current folder from RStudio
library(rstudioapi) # version 0.15
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr)
library(tidyr)
library(mvtnorm)
library(invgamma)
library(splines)
library(ggplot2)
theme_set(theme_bw(base_size = 14))
library(reshape2)
# Data:
#   is read in by readDta(), which is called right after it's defined.
# Data comes from the file  "data_protein.RData"
# After calling readDta() the design matrix and responses are saved
# in global variables "X, y"
# and C (=# columns=proteins), R (=# rows, i.e., patients)
# and p (dim of design vector).
# That is X is (R x p) etc.
#
#
# Prior:
# prior par's are set in a global variable "prior = list(..)"
# which is set right after reading in the data.
# In particular (m-eta, S-eta) are the normal moments of protein-specific
# spline coefficients eta_j; (m-xi, S-xi) the same for patient-specific xi_i.
# (a-eta, b-eta) are the PY par's for proteins;
# and (a-xi, b-xi) the same for patients.
#
#
# main:
# first sets up data structures for G_eta ~ PY(a,b) and G_xi ~ PY(a,b).
# Since we have y[ijt] ~ N(xi_i + x_i' eta_j, sig^2)
# we introduce an adjusted outcome
# yt = y-xi with mdpOffset(mdpXi) before updating eta's, and similarly
# yt = y-eta with mdpOffset(mdpEta) before updating xi's.
# The yt is a global variable.
# mdpInitClust(.) simply initializes the random partition of the eta_j's
# THe main MCMC is then
# for(it in 1:niter){
#   update eta
#   update xi
#   update sig^2
# }
#
#
# sim results:
# are saved in mcmcUpd().
# All it does is just update running totals Ey (for fitted curves),
# Eyp (for fitted curves *w/o* patient effects -- just proteins).
# Those are saved in "Ey.txt", "Ey2.txt"(for 2nd moments) etc.
#
#
# random partition:
# mdp$s are random partition cluster membership indicators
# (mdp=mdpEta, and mdp=mdpXi for proteins and patients, respectively).
# Those are saved in "sProt.txt" (just the first 250 proteins..),
# and "sPat.txt", respectively.
#
#
# Polya urn:
# see mdpUpdS(.) for the Polya urn for the PY processes.
# For clusters with cardinality < ns (=20)
# I marginalize wrt cluster specific parameters.
# That's a bit computation intensive (using candidate's formula).
# For ns>20 i condition on cluster-specific pars.
# Posterior uncertainty is  small it doesn't make a differcence for our data.
# It's a bit of a pain to keep track of marginal (or conditional, for ns>20)
# cluster-specific likelihood under alternative cluster assignments.
# To be checked further. Seems ok, also since the fitted curves seem ok.
#
#
# plots:
# in the end you find some plotting routines. First call "pltInit(.)",
# just to read in all the sim output from the files into global variables
# (so we don't have to read in for each plot).
# plt_reg(.) is an omnibus plotting funciton.
# Setting the arguments as desired you get whichever plots.
# See "paper_reg(.)" for what we did. Just ignore the "ps_reg(..)" funcitons.
# That's my personal ones to get the right graphics par's
# maxDiff_reg() find the proteins with largest difference across time.
set.seed(135)
source("SEP_fcts.R")
## global variables for the dta
out = readDta() ## make global vars for the data
