if(newTableAllocation < 0) {
##### COMPUTE THE CONTRIBUTION PART OF THE LOG ACCEPTANCE PROBABILITY
individualsToRemove = c((firstIndividuals[indexGroup]-1+indexCustomerGroup):lastIndividuals[indexGroup])
nPeopleAtTableCS = as.integer(table(factor(observationTableAllocation[-individualsToRemove], levels = 1:maxTableIndex))) # nPeopleAtTable - as.integer(table(factor(observationTableAllocation[individualsToRemove], levels = 1:maxTableIndex))) # as.integer(table(factor(observationTableAllocation_noJ, levels = 1:maxTableIndex))) # this is wrong since it removes twice the current observation from the count of people at table
nTablesCS = sum(nPeopleAtTableCS>0)
# compute nTablesInRestaurantCS
# freeTablesCS = (1:maxTableIndex)[nPeopleAtTableCS == 0]
# tableRestaurantAllocationCS = tableRestaurantAllocation
# tableRestaurantAllocationCS[freeTablesCS] = -1
# nTablesInRestaurantCS = sum(tableRestaurantAllocationCS==indexRestaurant)
# nTablesInRestaurantCS =
#   length(table(observationTableAllocation[-individualsToRemove][observationRestaurantAllocation[-individualsToRemove]==indexRestaurant]))
#
# compute log of P[X_ji=x|____]
if(dishesCountsCS[currentDish] == 0){ # X_ji = "new"
logP_CS = logP_CS +
log(theta0 + nDishesCS*sigma0) -
log(theta0 + nTablesCS)
nDishesCS = nDishesCS + 1
} else{
# compute number of tables serving the current dish in the whole franchise, excluding individualsToRemove
# nTablesServingCurrentDishCS = length(table(observationTableAllocation[-individualsToRemove][observationDishAllocation[-individualsToRemove] == currentDish]))
# alternative computation:
freeTablesCS = (1:maxTableIndex)[nPeopleAtTableCS == 0] # which(nPeopleAtTableCS == 0)
tablesValuesCS = tablesValues
tablesValuesCS[freeTablesCS] = -1
nTablesServingCurrentDishCS = sum(tablesValuesCS==currentDish)
# get the tables in current restaurant serving the current dish (excluding future observations)
# freeTablesCS = (1:maxTableIndex)[nPeopleAtTableCS == 0] # which(nPeopleAtTableCS == 0)
# tableRestaurantAllocationCS = tableRestaurantAllocation
# tableRestaurantAllocationCS[freeTablesCS] = -1
# tablesInRestaurantServingCurrentDishCS = (1:maxTableIndex)[((tableRestaurantAllocationCS == indexRestaurant)&(tablesValues == currentDish))]
# tablesValuesCS = tablesValues
# tablesValuesCS[freeTablesCS] = -1
#nTablesInRestaurantServingCurrentDishCS = length(tablesInRestaurantServingCurrentDish)
logP_CS = logP_CS +
log(nTablesServingCurrentDishCS - sigma0) -
log(theta0 + nTablesCS)
}
### update quantities of "current state"
# nPeopleInRestaurantCS = nPeopleInRestaurantCS + 1
dishesCountsCS[currentDish] = dishesCountsCS[currentDish] + 1
##### UPDATE GIBBS SAMPLING QUANTITIES
nTables = nTables + 1
if(nFreeTables > 0) { # pick the first free table
newTableAllocation = freeTables[1]
freeTables = freeTables[-1]
nFreeTables = nFreeTables - 1
nPeopleAtTable[newTableAllocation] = 1
tablesValues[newTableAllocation] = currentDish
tableRestaurantAllocation[newTableAllocation] = indexRestaurant # assign table to restaurant
} else { # create a new table
maxTableIndex = maxTableIndex + 1
newTableAllocation = maxTableIndex
nPeopleAtTable = c(nPeopleAtTable,1)
tablesValues = c(tablesValues,currentDish)
tableRestaurantAllocation = c(tableRestaurantAllocation,indexRestaurant) # assign table to restaurant
}
} else{ # the sampled table is already occupied in the restaurant --> just update the relevant quantities
nPeopleAtTable[newTableAllocation] = nPeopleAtTable[newTableAllocation] + 1
}
observationTableAllocation[indexCustomerGlobal] = newTableAllocation
indexCustomerGlobal = indexCustomerGlobal + 1
}
#### MH STEP
# we begin the MH proposal sampling
nRestMH = nRest
maxRestIndexMH = maxRestIndex
groupRestaurantAllocationMH = groupRestaurantAllocation
nGroupsInRestaurantMH = nGroupsInRestaurant
nGroupsInRestaurantMH[indexRestaurant] = nGroupsInRestaurantMH[indexRestaurant] - 1
nFreeRestaurantsMH = nFreeRestaurants
freeRestaurantsMH = freeRestaurants
if(nGroupsInRestaurantMH[indexRestaurant] == 0) {
nRestMH = nRestMH - 1
nFreeRestaurantsMH = nFreeRestaurantsMH + 1
freeRestaurantsMH = c(indexRestaurant,freeRestaurantsMH)
}
#### PROPOSE A NEW RESTAURANT AND TABLE ALLOCATION
#### sample restaurant
nonEmptyRest = (1:maxRestIndexMH)[nGroupsInRestaurantMH>0] # restaurants with at least one group assigned
possibleRestaurants = c(nonEmptyRest,-1)
probs =c(nGroupsInRestaurantMH[nonEmptyRest] - alpha, gamma + alpha*nRestMH)
newRestaurantAllocation = sample(possibleRestaurants,1,replace = F, prob = probs)
if(newRestaurantAllocation < 0) {
nRestMH = nRestMH + 1
if(nFreeRestaurantsMH > 0) { # pick the first free restaurant
newRestaurantAllocation = freeRestaurantsMH[1]
freeRestaurantsMH = freeRestaurantsMH[-1]
nFreeRestaurantsMH = nFreeRestaurantsMH - 1
nGroupsInRestaurantMH[newRestaurantAllocation] = 1
groupRestaurantAllocationMH[indexGroup] = newRestaurantAllocation # assign group to restaurant
} else { # create a new restaurant
maxRestIndexMH = maxRestIndexMH + 1
newRestaurantAllocation = maxRestIndexMH
nGroupsInRestaurantMH = c(nGroupsInRestaurantMH,1)
groupRestaurantAllocationMH[indexGroup] = newRestaurantAllocation # assign group to restaurant
}
} else{ # the sampled restaurants contains already some groups --> just update the relevant quantities
groupRestaurantAllocationMH[indexGroup] = newRestaurantAllocation # assign group to restaurant
nGroupsInRestaurantMH[newRestaurantAllocation] = nGroupsInRestaurantMH[newRestaurantAllocation] + 1
}
#### sample table allocation
# initialize MH quantities excluding current group j
maxTableIndexMH = maxTableIndex
observationTableAllocationMH = observationTableAllocation
observationTableAllocationMH[individualsInCurrentGroup] = -1
observationRestaurantAllocationMH = observationRestaurantAllocation_noJ # computed above
observationDishAllocationMH = observationDishAllocation
observationDishAllocationMH[individualsInCurrentGroup] = -1
nPeopleAtTableMH = as.integer(table(factor(observationTableAllocation[-individualsInCurrentGroup], levels = 1:maxTableIndex))) # nPeopleAtTable - as.integer(table(factor(observationTableAllocation[individualsInCurrentGroup], levels = 1:maxTableIndex))) # as.integer(table(factor(observationTableAllocation_noJ, levels = 1:maxTableIndex)))
nTablesMH = sum(nPeopleAtTableMH>0)
freeTablesMH = (1:maxTableIndexMH)[nPeopleAtTableMH == 0]
nFreeTablesMH = length(freeTablesMH)
tablesValuesMH = tablesValues
tablesValuesMH[freeTablesMH] = -1
tableRestaurantAllocationMH = tableRestaurantAllocation
tableRestaurantAllocationMH[freeTablesMH] = -1
# nPeopleInRestaurantMH = nPeopleInRestaurant_noJ # computed above
dishesCountsMH = dishesCounts_noJ
nDishesMH = nDishes - sum(dishesCountsMH==0)
# # nTablesServingCurrentDish = sum(tablesValues_noJ == observationDishAllocation[indexCustomerGlobal])
##### end computation of quantities without group J
# #### UPDATE TABLE CONFIGURATION IN THE CURRENT GROUP (analogous to the sampling from the full conditionals)
logP_MH = 0 # log of P[X_ji=x|____ S'] to be used in MH
for (indexCustomerGroup in 1:nByGroup[indexGroup]) { # this loop should be joined with the loop for the full conditionals!!!!!
indecesTablesInRestaurant = (1:maxTableIndexMH)[tableRestaurantAllocationMH==newRestaurantAllocation]
currentDish = observationDishAllocation[indexCustomerGlobalMH] # get the current dish
observationRestaurantAllocationMH[indexCustomerGlobalMH] = newRestaurantAllocation
nTablesInRestaurant = sum(tableRestaurantAllocationMH==newRestaurantAllocation)
indecesPossibleTables = (tablesValuesMH[indecesTablesInRestaurant] == observationDishAllocation[indexCustomerGlobalMH])
possibleTables = c(indecesTablesInRestaurant[indecesPossibleTables],-1)
probNewTable = (theta + sigma*nTablesInRestaurant)/(nTablesMH + theta0)
nTablesServingCurrentDishMH = sum(tablesValuesMH == observationDishAllocation[indexCustomerGlobalMH])
if(nTablesServingCurrentDishMH > 0) {
probNewTable = probNewTable*(nTablesServingCurrentDishMH - sigma0)
}
probs = c(nPeopleAtTableMH[indecesTablesInRestaurant][indecesPossibleTables] - sigma, probNewTable)
newTableAllocation = sample(possibleTables,1,replace = F, prob = probs)
if(newTableAllocation < 0) {
# compute log of P[X_ji=x|____S']
if(dishesCountsMH[currentDish] == 0){ # X_ji = "new"
logP_MH = logP_MH +
log(theta0 + nDishesMH*sigma0) -
log(theta0 + nTablesMH)
} else{
# compute number of tables serving the current dish in the whole franchise, excluding future individuals
nTablesServingCurrentDishMH = length(table(observationTableAllocationMH[observationDishAllocationMH == currentDish]))
# get the tables in current restaurant serving the current dish (excluding future observations)
# tablesInRestaurantServingCurrentDish = (1:maxTableIndexMH)[((tableRestaurantAllocationMH == newRestaurantAllocation)&(tablesValuesMH == currentDish))]
logP_MH = logP_MH +
log(nTablesServingCurrentDishMH - sigma0) -
log(theta0 + nTablesMH)
}
### update quantities of MH
# nPeopleInRestaurantMH = nPeopleInRestaurantMH + 1
dishesCountsMH[currentDish] = dishesCountsMH[currentDish] + 1
observationDishAllocationMH[indexCustomerGlobalMH] = currentDish
nTablesMH = nTablesMH + 1
if(nFreeTablesMH > 0) { # pick the first free table
newTableAllocation = freeTablesMH[1]
freeTablesMH = freeTablesMH[-1]
nFreeTablesMH = nFreeTablesMH - 1
nPeopleAtTableMH[newTableAllocation] = 1
tablesValuesMH[newTableAllocation] = observationDishAllocationMH[indexCustomerGlobalMH]
tableRestaurantAllocationMH[newTableAllocation] = newRestaurantAllocation # assign table to restaurant
} else { # create a new table
maxTableIndexMH = maxTableIndexMH + 1
newTableAllocationMH = maxTableIndexMH
nPeopleAtTableMH = c(nPeopleAtTableMH,1)
tablesValuesMH = c(tablesValuesMH,observationDishAllocationMH[indexCustomerGlobalMH])
tableRestaurantAllocationMH = c(tableRestaurantAllocationMH,newRestaurantAllocation) # assign table to restaurant
}
} else{ # the sampled table is already occupied in the restaurant --> just update the relevant quantities
nPeopleAtTableMH[newTableAllocation] = nPeopleAtTableMH[newTableAllocation] + 1
}
observationTableAllocationMH[indexCustomerGlobalMH] = newTableAllocation
# update indexCustomerGlobalMH
indexCustomerGlobalMH = indexCustomerGlobalMH + 1
}
pAccept = min(exp(logP_MH - logP_CS),1)
accept = runif(1) < pAccept
# print(pAccept)
if(accept == T){
nRest = nRestMH
maxRestIndex = maxRestIndexMH
groupRestaurantAllocation[indexGroup] = groupRestaurantAllocationMH[indexGroup]
nGroupsInRestaurant = nGroupsInRestaurantMH
nFreeRestaurants = nFreeRestaurantsMH
freeRestaurants = freeRestaurantsMH
maxTableIndex = maxTableIndexMH
observationTableAllocation = observationTableAllocationMH
observationRestaurantAllocation = observationRestaurantAllocationMH
nPeopleAtTable = nPeopleAtTableMH
nTables = nTablesMH
freeTables = freeTablesMH
nFreeTables = nFreeTablesMH
tablesValues = tablesValuesMH
tableRestaurantAllocation = tableRestaurantAllocationMH
}
##### END OF METROPOLIS HASTINGS STEP
### EDIT THIS PART TO UPDATE THE TABLES ALLOCATIONS FOR THE WHOLE GROUP
groupRestaurantAllocationAcrossGibbs[r, indexGroup] = groupRestaurantAllocation[indexGroup]
observationTableAllocationAcrossGibbs[r,firstIndividuals[indexGroup]:lastIndividuals[indexGroup]] =
observationTableAllocation[firstIndividuals[indexGroup]:lastIndividuals[indexGroup]]
}
### SAMPLE TABLE AND DISH FOR OUT-OF-SAMPLE OBSERVATIONS
# THIS HAS BEEN MOVED OUT OF THE LOOP FOR THE MOMENT
}
nTablesServingCurrentDish
sigma0
log(nTablesServingCurrentDishCS - sigma0)
nTablesServingCurrentDishCS
# Load relevant libraries, functions and data ----------------------------------
rm(list=ls())
# Set the working directory to the current folder
# Code to set the working directory to the current folder from RStudio
library(rstudioapi) # version 0.15
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(DirichletReg)
library(invgamma)
library(tidyr)
library(dplyr)
library(truncnorm)
library(scales)
library(viridisLite)
library(ggplot2)
theme_set(theme_bw(base_size = 14))
library(RColorBrewer)
library(salso)
library(ggpubr)
library(salso)
library(reshape2)
###################################################N
## create global variables for data & field dimensions
# Read and clean the data
###################################################N
## read in data
read.dta  <- function()
{##read-in and clean data
Dietswap_dataset <- readRDS("Data-and-Results/Dietswap_dataset.RDS")
data = Dietswap_dataset[Dietswap_dataset$timepoint==1,]
data2=pivot_wider(data[,1:3],names_from = Sample,values_from = Abundance)
na_in_rows=sapply(1:dim(data2)[1],function(i) sum(data2[i,]==0))
data3=data.frame(data2[na_in_rows!=38,]) #remove all na rows
Y0=data3[,2:39]                ## separate OTU counts
rownames(Y0)=data3[,1]         ## and OTU names as row names..
n <- dim(Y0)[2]                ## n patients (J in the paper's notation)
remove_sparse = F
if(remove_sparse){
drop = apply(Y0==0,1,sum)>n/4  ## drop all OTUS with more than n/4 0's
Y  <- Y0[!drop,]               ## dropping 20 of the originally 119 OTU's
## now total # 0's is only 5.
} else {
Y  <- Y0
}
gammas=colMeans(Y)
Y=sapply(1:n,function(i) as.matrix(Y)[,i]/gammas[i])
y=log(Y+0.05)                  ## log transform (we only have 5 0's)
return(list(y=y,Y=Y)) ## return both - the log transf "y" and raw "Y"
}
## create global variables for data & fied dimensions
out <- read.dta() ## maybe as.matrix(..)?? See if needed..
y <-   out$y    ## log transf
Y  <-  out$Y    ## abs scale
N <- dim(y)[1]*dim(y)[2]  # global var's (I*J in the paper)
n <- dim(y)[2] # # patients (J in the paper)
B <- dim(y)[1] # # OTU's (I in the paper)
##### max size of the subject (K) and OTU (L) partition
K = 10
L = 15
## hyperprior pars
alpha = 1      ## for GEM prior on pi
beta  = 1      ## GEM prior on w[k], k = 1, ..., K
## hyperpars (use "empirical Bayes" i.e., match sample moments)
mu0  = mean(as.matrix(y))                  ## mu_l ~ N(mu0, sig0)
sig0 = 3  ## SD!
a0 = 3                                    ## sig2_l ~ Ga(a0/2, b0/2)
b0 = var(c(y))/4
source("SEP_fcts.R")
############ exploratory plots ###########
Exploratory = FALSE
if(Exploratory){
Dietswap_dataset <- readRDS("Data-and-Results/Dietswap_dataset.RDS")
data = Dietswap_dataset[Dietswap_dataset$timepoint==1,]
data2=pivot_wider(data[,1:3],names_from = Sample,values_from = Abundance)
na_in_rows=sapply(1:dim(data2)[1],function(i) sum(data2[i,]==0))
data3=data.frame(data2[na_in_rows!=38,]) #remove all na rows
y=data3[,2:39]
gammas=colMeans(y)
y=sapply(1:n,function(i) as.matrix(y)[,i]/gammas[i])
y1=y[order(rowSums(y),decreasing = T),]
data_country=pivot_wider(data[,c(1:2,6)], names_from = Sample,
values_from = nationality)
data_country=data_country[1,2:dim(data_country)[2]]
data_country=as.matrix(data_country)[1,]
names(data_country)=NULL
data_label=data_country
data_label[data_country=='AFR']=1
data_label[data_country=='AAM']=2
data_label2 = data_country
data_label2[data_country=='AFR'] = 'AF'
data_label2[data_country=='AAM'] = 'AA'
y_af=as.matrix(y1[,data_country=='AFR'])
y_am=as.matrix(y1[,data_country=='AAM'])
pdf_af=rowSums(y_af)/sum(rowSums(y_af))
pdf_am=rowSums(y_am)/sum(rowSums(y_am))
pdf_all=rowSums(y1)/sum(rowSums(y1))
P = ggplot()+
geom_line(aes(0:119,c(0,cumsum(pdf_af)),col='AF'))+
geom_line(aes(0:119,c(0,cumsum(pdf_am)),col='AA'))+
geom_line(aes(0:119,c(0,cumsum(pdf_all)),col='ALL'))+
labs(x='OTU', y="Cumulative Relative Frequencies")+
theme(legend.position = 'right')+  labs(color=NULL)
ggsave(plot=P, file ="Image/Emp_CDF_OTU.pdf",
width=16, height=12, units = 'cm')
dfplot_af=cbind(1:119,rowMeans(y_af)) %>% data.frame()
colnames(dfplot_af) =  c("OTU","Abundace")
P1 = ggplot(dfplot_af,aes(OTU,Abundace))+
geom_bar(stat='identity')+
labs(x='OTU (AF)',y='Scaled Abundance')
ggsave(plot=P1, file ="Image/normedcount-hist-af.pdf",
width=5, height=3.5, units = 'in')
dfplot_am=cbind(1:119,rowMeans(y_am)) %>% data.frame()
colnames(dfplot_am) =  c("OTU","Abundace")
P2 = ggplot(dfplot_am,aes(OTU,Abundace))+
geom_bar(stat='identity')+
labs(x='OTU (AA)',y='Scaled Abundance')
ggsave(plot=P2, file ="Image/normedcount-hist-am.pdf",
width=5, height=3.5, units = 'in')
#######plot dendograms based on hierarchical clustering
colvar <- function(y){
return(sapply(1:dim(y)[2],function(j) var(y[,j])))
}
rowvar <- function(y){
return(sapply(1:dim(y)[1],function(i) var(y[i,])))
}
rvy=rowvar(y)
yhc=y[order(rvy,decreasing = T)<=10,]
rownames(yhc)=rownames(y)[order(rvy,decreasing = T)<=10]
colnames(yhc)=data_label
d <- dist(yhc, method = "euclidean")
hc2 <- hclust(d, method = "complete" )
plot(hc2,cex=0.5,main='')
d <- dist(t(yhc), method = "euclidean")
hc2 <- hclust(d, method = "complete" )
plot(hc2,cex=0.5,main='')
dend_expr <- as.dendrogram(hc2)
tree_labels <- dendro_data(dend_expr, type = "rectangle")
tree_labels$labels <- cbind(tree_labels$labels, Subject = as.factor(data_label2))
P = ggplot() +
geom_segment(data =segment(tree_labels), aes(x=x, y=y, xend=xend, yend=yend))+
geom_segment(data = tree_labels$segments %>%
filter(yend == 0) %>%
left_join(tree_labels$labels, by = "x"),
aes(x=x, y=y.x, xend=xend, yend=yend, color = Subject)) +
geom_text(data = label(tree_labels),
aes(x=x, y=y, label=label, hjust=-1), size=2) +
coord_flip() +
scale_y_reverse(expand=c(0.2, 0)) +
scale_colour_brewer(palette = "Dark2") +
theme_dendro()+
theme(legend.title=element_blank())
ggsave(plot=P, file ="Image/otu-subject-hc.pdf",
width=5, height=3.5, units = 'in')
}
# Plot Results
# Run analysis, save results and produce some plots
Run_MCMC = FALSE
if(Run_MCMC){
ex(niter  = 1e4, #iteration MCMC
niter0 = 1e3, # estimating Sj after niter0 iterations and stop updating Sj
niter1 = 2e3 # just for plot
)
}
# Load output
pi    <- read.myfile("Data-and-Results/pi.txt")
Sj    <- read.myfile("Data-and-Results/Sj.txt")
w     <- read.myfile("Data-and-Results/w.txt", K, L) # w[k, iter, l]
mki   <- read.myfile("Data-and-Results/mki.txt", K)
mu    <- read.myfile("Data-and-Results/mu.txt")
sig2  <- read.myfile("Data-and-Results/sig.txt")
# nk=sapply(1:K,function(k) sum(Sj==k))
nk    <- read.myfile("Data-and-Results/nk.txt")
# nl=sapply(1:L,function(l) sum(c(mki)==l))
nl    <- read.myfile("Data-and-Results/nl.txt")
ll    <- read.myfile("Data-and-Results/logl.txt")
nkp   <- sum(nk>0)
nlp   <- sum(nl>0)
# summ  <- c(iter, nkp, nlp, nk, nl, ll, pmki)
summ  <- read.myfile("Data-and-Results/iter.txt")
niter <- dim(w)[2]
point_SJ = salso::salso(Sj, nRuns = 100, maxZealousAttempts = 100, loss=VI())
#plot
# Distributional clusters by OTU
Dietswap_dataset <- readRDS("Data-and-Results/Dietswap_dataset.RDS")
data = Dietswap_dataset[Dietswap_dataset$timepoint==1,]
data2=pivot_wider(data[,1:3],names_from = Sample,values_from = Abundance)
na_in_rows=sapply(1:dim(data2)[1],function(i) sum(data2[i,]==0))
data3=data.frame(data2[na_in_rows!=38,]) #remove all na rows
data=data3[,2:39]
# This plot is made adapting the code in Fradenti Github rep CommonAtomModel
cluster_per_obs <- rep(point_SJ, rep(nrow(data), ncol(data)))
LL <- apply(data,2,function(x) cumsum(sort((x),decreasing = T)) /sum((x)) )
dim(LL)
LL2 <- reshape2::melt(LL)
LL2 <- cbind(LL2,cluster_per_obs)
LL2 <- as_tibble(LL2) %>% mutate(cluster_per_obs=paste0("",cluster_per_obs))
P = ggplot(LL2)+
geom_hline(yintercept = c(0,1),lty=3)+
geom_line(aes(x=Var1,y=value,group=Var2,col=as.factor(cluster_per_obs)),
lwd=.5, alpha=.2)+
geom_point(aes(x=Var1,y=value,group=Var2,col=as.factor(cluster_per_obs)),
alpha=.9)+
theme_bw()+scale_color_manual("Subject\nCluster",
values = c(2,4,1,6,5)) + ylim(0, 1)+
xlab("Taxa sorted by count")+ylab("Cumulative Relative Frequncy")
P
#### Plot heatmaps
point_SJ = salso::salso(Sj, nRuns = 100, maxZealousAttempts = 100, loss=VI())
if(length(unique(point_SJ))!=3){
print("Adapt plot code for number of clusters different than 3")
stop()}
# TBD mki to be corrected
niter  = 1e4 #iteration MCMC
niter0 = 1e3 # estimating Sj after niter0 iterations and stop updating Sj
niter1 = 2e3 # just for plot
mki_all <- read.myfile("Data-and-Results/mki.txt",K)
summ = read.myfile("Data-and-Results/iter.txt")
it = summ[,1]
col = brewer.pal(9, name="YlOrRd")
M  <- nrow(mki_all)
M1 = which(it >= niter1)[1]
mki  <- array(0,dim=c(K,B,length(M1:M)))
idx = 0
for(m in M1:M){
idx = idx + 1
for(b in 1:B){
mki[,b,idx] = mki_all[m, (b-1)*K+(1:K)]
}
}
PSM1 = psm(t(mki[1,,]))
PSM2 = psm(t(mki[2,,]))
PSM3 = psm(t(mki[3,,]))
# Reorder rows and columns (observations) of a dissimilarity matrix intra groups
# and possibly reorder also the groups (batch of observations)
reorder_dismat_j <-  function(dismat, groups, order.groups=NULL){
# Use correlation between variables as distance
order.dis   = integer(0)
J           = length(unique(groups))
if(is.null(order.groups)){
order.j   = 1:J
} else {
order.j   = order.groups
}
for (j in order.j){
groups.j  = which(groups==j)
dd        = as.dist((1-dismat[groups.j, groups.j])/2)
hc        = hclust(dd)
order.dis = c(order.dis, hc$order+length(order.dis))
}
dismat      = dismat[order.dis, order.dis]
dismat      = dismat[nrow(dismat):1,]
}
Plot_heat <- function(dissimlar_stable = dissimlar_stable,
I          = B){
dismat      = round(dissimlar_stable, 2)
dismat      = reorder_dismat_j(dismat,groups=rep(1,I))
plot_dismat = reshape2::melt(dismat)
ggplot(data=plot_dismat, aes(x=factor(Var1), y=factor(Var2), fill=value)) +
geom_tile() + theme_bw()+
scale_y_discrete(limits=floor(seq(I, 1)),
breaks = floor(seq(1, I, length.out = 9)),
labels = floor(seq(1, I, length.out = 9))) +
scale_x_discrete(breaks = floor(seq(1, I, length.out = 9)),
labels = floor(seq(1, I, length.out = 9))) +
xlab("OTU") + ylab("OTU") +
scale_fill_gradientn(colours = c("white", "yellow", "red"),
values = rescale(c(0, 0.5, 1)),
space = "Lab", name="") +
theme(legend.position = "right", text = element_text(size=20))
}
P1 = Plot_heat(PSM1, B) + xlab("") + theme(legend.position="none")
P2 = Plot_heat(PSM2, B) + ylab("") + theme(legend.position="none")
P3 = Plot_heat(PSM3, B) + xlab("") + ylab("")
PHeat <- ggarrange(P1, P2, P3, labels = c("1", "2", "3"), nrow=1,
widths = c(1, 1, 1.2))
PHeat
Point_Mki1 = salso::salso(t(mki[1,,]), nRuns = 100,
maxZealousAttempts = 100, loss=VI())
Point_Mki2 = salso::salso(t(mki[1,,]), nRuns = 100,
maxZealousAttempts = 100, loss=VI())
Point_Mki3 = salso::salso(t(mki[1,,]), nRuns = 100,
maxZealousAttempts = 100, loss=VI())
Zmelts <- rbind(
reshape2::melt(y[order(Point_Mki1),point_SJ==1]) %>%
mutate(point_SJ = "C-1"),
reshape2::melt(y[order(Point_Mki2),point_SJ==2]) %>%
mutate(point_SJ = "C-2") #,
# cbind(Var1=1:length(Point_Mki3), Var2 = rep(1, length(Point_Mki3)),
#       reshape2::melt(Z[order(Point_Mki3),point_SJ==3])) %>%
#   mutate(point_SJ = "C-3")
)
Zmelts$Var1 = as.factor(as.integer(Zmelts$Var1))
P = ggplot(Zmelts,aes(Var2,Var1,fill=value)) +
geom_raster() + theme_bw()+
facet_wrap(.~point_SJ, scales = "free")+
scale_fill_viridis_c()+
theme(axis.text.x = element_blank(),
axis.text.y = element_blank())+
labs(y= "OTU", x = "Subject")
P
