idx_temp = which((D[,1] == s_temp) & (ind == i_temp))
if (length(idx_temp) > 0){
delta_old[s_temp,i_temp] = min(tau[idx_temp])/2
delta_dat[idx_temp] = delta_old[s_temp,i_temp]
}
else{
delta_old[s_temp,i_temp] = 1E-3
}
}
for(j in 1:d_j[2]){
beta_mu_old[,s_temp,j] = rep(0.5*log(mean(tau[D[,1] == s_temp])) - log(sd(tau[D[,1] == s_temp])), J)
b_old[j] = 0.5*log(mean(tau[D[,1] == s_temp])) - log(sd(tau[D[,1] == s_temp]))
B_beta_b_dat[which((D[,1] == s_temp)),j] = b_old[j]
}
}
low_bound_mu = min(beta_mu_old) - 1.5
upp_bound_mu = max(beta_mu_old) + 1
beta_mu_star_prop = array(NA, dim = c(J, Z_max))
beta_mu_u_old = array(0, dim = c(n_ind, J))
sigma2_1_mu_old = 0.005
sigma2_mu_us_old = 0.005
sigma2_mu_ua_old = 0.005
sigma2_b_ua_old = 0.005
# Message passing structures
beta_mess = array(NA, dim = c(J, dim_HB))
z_old = list()
for (j in 1:d_j[1]){
z_old[[j]] = array(NA, dim = c(d_j[2], J))
for (jj in 1:d_j[2]){
if (j == jj){
z_old[[j]][jj,] = j
}
else{
z_old[[j]][jj,] = sample((d_j[1] + 1):Z_max, 1)
}
}
}
z_temp = do.call(rbind, z_old)
beta_mu_star_old = array(NA, dim = c(J, Z_max))
for (i in 1:Z_max){
beta_mu_star_old[,i] = beta_mu_old[1,idx_xy[which(apply(z_temp == i, 1, prod) == 1)[1],1],
idx_xy[which(apply(z_temp == i, 1, prod) == 1)[1],2]]
}
rand_mat = array(rnorm(prod(d_j)), dim = d_j)
idx_succ = which(rand_mat == diag(rand_mat))
v_old = array(NA, dim = c(d_j[2], J))
z_prop = list()
# Transition dynamics objects
alpha_S_old = alpha_F_old = 1
Q_S_old = array(NA, dim = c(Z_max, Z_max))
Q_F_old = array(NA, dim = c(Z_max, Z_max))
pi_S_0 = rdirichlet(rep(alpha_S_old / Z_max, Z_max) +
table_int(z_temp[idx_succ,1], Z_max))
pi_F_0 = rdirichlet(rep(alpha_F_old / Z_max, Z_max) +
table_int(z_temp[-idx_succ,1], Z_max))
tr_count_S = count_assign(z_temp[idx_succ,], Z_max)
tr_count_F = count_assign(z_temp[-idx_succ,], Z_max)
for (h in 1:Z_max){
Q_S_old[h,] = rdirichlet(rep(alpha_S_old / Z_max, Z_max) + tr_count_S[h,])
Q_F_old[h,] = rdirichlet(rep(alpha_F_old / Z_max, Z_max) + tr_count_F[h,])
}
# Auxiliary variables
prob_mat = array(NA, dim = c(dim_HB, dim_HB))
prob_vec = array(NA, dim = dim_HB)
beta_b_u_chain = array(NA, dim = c(samp_size, n_ind))
# MH proposal parameters
sd_MH_delta = array(0.3, dim = c(d_j[1], n_ind))
sd_MH_beta_mu = 0.4
sd_MH_beta_b = array(0.05, dim = d_j[2])
acc_b = array(0, dim = d_j[2])
sd_beta_mu_u =  array(0.4, dim = c(n_ind, J))
sd_beta_b_u =  array(0.4, dim = n_ind)
acc_delta = array(0, dim = c(d_j[1], n_ind))
acc_beta_mu_u = array(0, dim = c(n_ind, J))
acc_beta_b_u = array(0, dim = n_ind)
n_batch = 0
# Auxiliary variables
B_beta_mu_dat = array(0, dim = c(n, d_j[1]))
B_beta_b_dat = array(b_old, dim = c(n, d_j[1]))
B_beta_mu_u_dat = array(0, dim = c(n, d_j[1]))
B_beta_b_u_dat = array(0, dim = c(n, d_j[1]))
mu_dat = exp(B_beta_mu_dat + B_beta_mu_u_dat)
b_dat = exp(B_beta_b_dat + B_beta_b_u_dat)
# Gibbs Sampler
it = 1
pb = txtProgressBar(style = 3)
Niter
1
iter=1
# (0) Adaptively tune the MH variance for the proposals of \delta_{s, i},
# beta_u_mu, beta_u_b
if (iter %% 20 == 0){
n_batch = n_batch + 1
delta_n = min(0.01, n_batch^(-0.5))
for (i in 1:n_ind){
if (acc_beta_b_u[i]/iter > 0.44){
sd_beta_b_u[i] = exp(log(sd_beta_b_u[i]) + delta_n)
}
else{
sd_beta_b_u[i] = exp(log(sd_beta_b_u[i]) - delta_n)
}
for (x_temp in 1:d_j[1]){
if (acc_delta[x_temp,i]/iter > 0.44){
sd_MH_delta[x_temp,i] = exp(log(sd_MH_delta[x_temp,i]) + delta_n)
}
else{
sd_MH_delta[x_temp,i] = exp(log(sd_MH_delta[x_temp,i]) - delta_n)
}
}
for (k in 1:J){
if (acc_beta_mu_u[i,k]/iter > 0.44){
sd_beta_mu_u[i,k] = exp(log(sd_beta_mu_u[i,k]) + delta_n)
}
else{
sd_beta_mu_u[i,k] = exp(log(sd_beta_mu_u[i,k]) - delta_n)
}
}
}
for (d2 in 1:d_j[2]){
if (acc_b[d2]/iter > 0.44){
sd_MH_beta_b[d2] = exp(log(sd_MH_beta_b[d2]) + delta_n)
}
else{
sd_MH_beta_b[d2] = exp(log(sd_MH_beta_b[d2]) - delta_n)
}
}
}
# (1) Update of the delta parameter: \delta_{s,i}: MH with log normal
#     proposal
for (s_temp in 1:d_j[1]){
for (i_temp in 1:n_ind){
idx_temp = which((D[,1] == s_temp) & (ind == i_temp))
tau_temp = tau[idx_temp]
cens_temp = cens[idx_temp]
D_temp = D[idx_temp,]
# log-normal proposal distribution centered on the current value
delta_prop = exp(rnorm(1, log(delta_old[s_temp,i_temp]), sd_MH_delta[s_temp,i_temp]))
while (delta_prop > min(tau[idx_temp])){
delta_prop = exp(rnorm(1, log(delta_old[s_temp,i_temp]), sd_MH_delta[s_temp,i_temp]))
}
loglik_prop = log_likelihood(tau_temp, mu_dat[idx_temp,],
b_dat[idx_temp,],
rep(delta_prop, length(idx_temp)),
cens_temp, D_temp, TRUE)
loglik_old = log_likelihood(tau_temp, mu_dat[idx_temp,],
b_dat[idx_temp,],
rep(delta_old[s_temp,i_temp], length(idx_temp)),
cens_temp, D_temp, TRUE)
alpha_acc = min(0, loglik_prop + log(delta_prop) -
loglik_old - log(delta_old[s_temp,i_temp]))
l_u = log(runif(1))
if (l_u < alpha_acc){
delta_old[s_temp,i_temp] = delta_prop
delta_dat[idx_temp] = delta_old[s_temp,i_temp]
acc_delta[s_temp,i_temp] = acc_delta[s_temp,i_temp] + 1
}
}
}
# (2) Update of mu parameters: \mu_{x,y}^{(i)}(t)
for (k in 1:J){ # loop over locations
if (k == 1){ # only data at t = 1 influence the first coefficient
idx_time = T_min
}
else if (k == J){ # only data at t = T influence the last coefficient
idx_time = T_max
}
else { # data at t = {k-1,k} influence the kth coefficient
idx_time = (k-1):k
}
for (h in 1:Z_max){ # loop over possible latent values
# tuples (combinations of covariates) that are clustered together via
# the latent h
idx_cov = matrix(idx_xy[which(z_temp[,k] == h),], length(which(z_temp[,k] == h)), p)
X_1k = unique(idx_cov[,1]) # all possible values of x in this cluster
X_2k = unique(idx_cov[,2]) # all possible values of y in this cluster
if (length(X_1k) > 0){ # h \in \mathcal{Z}_{j,k}: posterior update
# Pick data with covariate levels of x clustered in group h and
# at the correct locations
idx_i = which( (D[,1] %in% X_1k) & (time %in% idx_time) ) # AS: what about X_2k?
tau_temp = tau[idx_i]
cens_temp = cens[idx_i]
D_temp = D[which((D[,1] %in% X_1k) & (time %in% idx_time)),] # AS: what about X_2k?
# Normal proposal distribution centered on the current value
if (k == 1){
beta_mu_star_prop[,h] = beta_mu_star_old[,h]
beta_mu_star_prop[k,h] = rnorm(1, beta_mu_star_old[k,h], sd_MH_beta_mu)
}
# Normal proposal from the prior
else if (k == J){
beta_pre1 = beta_mu_star_old[k-1, unique(z_temp[which(z_temp[,k] == h),k-1])]
beta_mu_star_prop[,h] = beta_mu_star_old[,h]
beta_mu_star_prop[k,h] = rnorm(1, beta_pre1, sqrt(sigma2_1_mu_old))
}
# Normal proposal from the prior
else {
beta_pre1 = beta_mu_star_old[k-1, unique(z_temp[which(z_temp[,k] == h),k-1])]
beta_mu_star_prop[,h] = beta_mu_star_old[,h]
beta_mu_star_prop[k,h] = rnorm(1, beta_pre1, sqrt(sigma2_1_mu_old))
}
B_beta_mu_prop_dat = B_beta_mu_dat[idx_i,]
# Modify the proposed values in the corresponding positions
for (hh in 1:nrow(idx_cov)){
B_beta_mu_prop_dat[which(D_temp[,1] == idx_cov[hh,1]),idx_cov[hh,2]] =
B[idx_i[which(D_temp[,1] == idx_cov[hh,1])],] %*% beta_mu_star_prop[,h]
}
# This is the proposed value for \b_{x,y}^{(i)}(t), \mu_{x,y}^{(i)}(t)
mu_prop_dat = exp(B_beta_mu_prop_dat + B_beta_mu_u_dat[idx_i,])
if (k == 1){
beta_post1 = beta_mu_star_old[k+1, unique(z_temp[which(z_temp[,k] == h),k+1])]
logpost_prop = log_likelihood(tau_temp, mu_prop_dat,
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_prop[k,h] - beta_post1)^2)
logpost_old = log_likelihood(tau_temp, mu_dat[idx_i,],
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_old[k,h] - beta_post1)^2)
}
else if (k == J){
logpost_prop = log_likelihood(tau_temp, mu_prop_dat,
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE)
logpost_old = log_likelihood(tau_temp, mu_dat[idx_i,],
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE)
}
else {
beta_post1 = beta_mu_star_old[k+1, unique(z_temp[which(z_temp[,k] == h),k+1])]
logpost_prop = log_likelihood(tau_temp, mu_prop_dat,
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_prop[k,h] - beta_post1)^2)
logpost_old = log_likelihood(tau_temp, mu_dat[idx_i,],
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_old[k,h] - beta_post1)^2)
}
alpha_acc = min(0, logpost_prop - logpost_old)
l_u = log(runif(1))
if (l_u < alpha_acc){
beta_mu_star_old[k,h] = beta_mu_star_prop[k,h]
B_beta_mu_dat[idx_i,] = B_beta_mu_prop_dat
mu_dat[idx_i,] = mu_prop_dat
}
}
else { # h \notin \mathcal{Z}_{1,k}: prior sampling
beta_mu_star_old[k,h] = runif(1, low_bound_mu, upp_bound_mu)
}
}
}
# 2(a) Update the constant boundary parameters
tau_temp = tau
cens_temp = cens
D_temp = D
for (d2 in 1:d_j[2]){
b_prop = rnorm(1, b_old[d2], sd_MH_beta_b[d2])
# Modify the proposed values in the corresponding positions
B_beta_b_prop_dat = B_beta_b_dat
B_beta_b_prop_dat[,d2] = b_prop
# This is the proposed value for b
b_prop_dat = exp(B_beta_b_prop_dat)
logpost_prop = log_likelihood(tau_temp, mu_dat,
b_prop_dat, delta_dat,
cens_temp, D_temp, TRUE)
logpost_old = log_likelihood(tau_temp, mu_dat,
b_dat, delta_dat,
cens_temp, D_temp, TRUE)
alpha_acc = min(0, logpost_prop - logpost_old)
l_u = log(runif(1))
if (l_u < alpha_acc){
b_old[d2] = b_prop
# b_dat = b_prop_dat
B_beta_b_dat = B_beta_b_prop_dat
b_dat = b_prop_dat
acc_b[d2] = acc_b[d2] + 1
}
}
# (3) Update the cluster assignments
for (x_temp in 1:d_j[1]){ # loop over possible latent values # AS: why only 1
beta_mess = array(-Inf, dim = c(J, dim_HB))
beta_mess[J,] = 1/dim_HB
v_old[,J] = H_ball_unif(z_old[[x_temp]][,J], S = Z_max, r = r_HB)
z_prop[[J]] = H_ball(v_old[,J], S = Z_max, r = r_HB)
for (k in (J - 1):1){
idx_i = which( (time == k) & (D[,1] == x_temp) )
tau_temp = tau[idx_i]
cens_temp = cens[idx_i]
D_temp = D[idx_i,]
# (i) Sample the auxiliary variables
v_temp = H_ball(z_old[[x_temp]][,k], S = Z_max, r = r_HB)
probs = rep(-Inf, dim_HB)
for (h in 1:dim_HB){
B_beta_mu_prop_dat = B_beta_mu_dat[idx_i,]
B_beta_mu_prop_dat = 0.5 * (beta_mu_star_old[k,v_temp[,h]] +
beta_mu_star_old[k+1,z_old[[x_temp]][,k+1]])
mu_dat_prop = exp(t(B_beta_mu_prop_dat + t(B_beta_mu_u_dat[idx_i,])))
probs[h] = g_HB(log_likelihood(tau_temp, mu_dat_prop, b_dat[idx_i,],
delta_dat[idx_i], cens_temp, D_temp, TRUE))
}
probs = as.numeric(normalise_log(probs))
v_old[,k] = v_temp[,sample(1:dim_HB, 1, prob = probs)]
z_prop[[k]] = H_ball(v_old[,k], S = Z_max, r = r_HB)
# (ii) Pass messages backwards only in the restricted state space given
#      by the slice
z_kp1_temp = which(beta_mess[k+1,] > 0)
prob_mat = array(-Inf, dim = c(dim_HB, dim_HB))
for (h1 in z_kp1_temp){
for (h2 in 1:dim_HB){
B_beta_mu_prop_dat = B_beta_mu_dat[idx_i,]
B_beta_mu_prop_dat_1 = 0.5 * (beta_mu_star_old[k,z_prop[[k]][,h2]] +
beta_mu_star_old[k+1,z_prop[[k+1]][,h1]])
B_beta_mu_prop_dat_2 = 0.5 * (beta_mu_star_old[k,v_old[,k]] +
beta_mu_star_old[k+1,z_prop[[k+1]][,h1]])
mu_dat_prop_1 = exp(t(B_beta_mu_prop_dat_1 + t(B_beta_mu_u_dat[idx_i,])))
mu_dat_prop_2 = exp(t(B_beta_mu_prop_dat_2 + t(B_beta_mu_u_dat[idx_i,])))
prob_mat[h2,h1] = log(beta_mess[k+1,h1]) -
0.5 / sigma2_1_mu_old * sum((beta_mu_star_old[k,z_prop[[k]][,h2]] -
beta_mu_star_old[k+1,z_prop[[k+1]][,h1]])^2) +
log_likelihood(tau_temp, mu_dat_prop_1, b_dat[idx_i,],
delta_dat[idx_i], cens_temp, D_temp, TRUE) +
g_HB(log_likelihood(tau_temp, mu_dat_prop_2, b_dat[idx_i,],
delta_dat[idx_i], cens_temp, D_temp, TRUE)) +
sum(log(Q_F_old[cbind(z_prop[[k]][-x_temp,h2],z_prop[[k+1]][-x_temp,h1])])) +
log(Q_S_old[z_prop[[k]][x_temp,h2],z_prop[[k+1]][x_temp,h1]])
}
}
if ( sum(is.infinite(sum_rows_log(prob_mat))) == dim_HB){
beta_mess[k,] = 1/dim_HB
}
else{
beta_mess[k,] = as.numeric(sum_rows_log(prob_mat))
beta_mess[k,] = as.numeric(normalise_log(beta_mess[k,]))
}
}
# (iii) Sample states forward (only on allowed states)
idx_fail = (1:d_j[2])[-x_temp]
# Sample z_1
prob_vec = log(beta_mess[1,]) + log(pi_S_0[z_prop[[1]][x_temp,]]) +
colSums(matrix(log(pi_F_0[z_prop[[1]][-x_temp,]]), d_j[2] - 1, dim_HB))
prob_vec = as.numeric(normalise_log(prob_vec))
idx_samp = sample(1:dim_HB, 1, FALSE, prob_vec)
z_old[[x_temp]][,1] = z_prop[[1]][,idx_samp]
# Sample z_k
for (k in 2:J){
idx_km1 = which( (time == k - 1) & (D[,1] == x_temp) )
tau_temp = tau[idx_km1]
cens_temp = cens[idx_km1]
D_temp = D[idx_km1,]
prob_vec = log(beta_mess[k,]) +
log(Q_S_old[cbind(z_old[[x_temp]][x_temp,k-1], z_prop[[k]][x_temp,])])
for (kkk in idx_fail){
prob_vec = prob_vec + log(Q_F_old[cbind(z_old[[x_temp]][kkk,k-1], z_prop[[k]][kkk,])])
}
for (z_k_temp in which(is.finite(prob_vec))){
B_beta_mu_prop_dat = B_beta_mu_dat[idx_km1,]
B_beta_mu_prop_dat_1 = 0.5 * (beta_mu_star_old[k-1,z_old[[x_temp]][,k-1]] +
beta_mu_star_old[k,z_prop[[k]][,z_k_temp]])
B_beta_mu_prop_dat_2 = 0.5 * (beta_mu_star_old[k-1,v_old[,k-1]] +
beta_mu_star_old[k,z_prop[[k]][,z_k_temp]])
mu_dat_prop_1 = exp(t(B_beta_mu_prop_dat_1 + t(B_beta_mu_u_dat[idx_km1,])))
mu_dat_prop_2 = exp(t(B_beta_mu_prop_dat_2 + t(B_beta_mu_u_dat[idx_km1,])))
prob_vec[z_k_temp] = prob_vec[z_k_temp] -
0.5 / sigma2_1_mu_old * sum((beta_mu_star_old[k-1,z_old[[x_temp]][,k-1]] -
beta_mu_star_old[k,z_prop[[k]][,z_k_temp]])^2) +
log_likelihood(tau_temp, mu_dat_prop_1, b_dat[idx_km1,],
delta_dat[idx_km1], cens_temp, D_temp, TRUE) +
g_HB(log_likelihood(tau_temp, mu_dat_prop_2, b_dat[idx_km1,],
delta_dat[idx_km1], cens_temp, D_temp, TRUE))
}
prob_vec = as.numeric(normalise_log(prob_vec))
idx_samp = sample(1:dim_HB, 1, FALSE, prob_vec)
z_old[[x_temp]][,k] = z_prop[[k]][,idx_samp]
}
# (4) Assign the cluster specific curves f_{\mu}
for (y_temp in 1:d_j[2]){
beta_mu_old[,x_temp,y_temp] = beta_mu_star_old[cbind(1:J, z_old[[x_temp]][y_temp,])]
}
B_beta_mu_dat[(D[,1] == x_temp),] = B[D[,1] == x_temp,] %*% beta_mu_old[,x_temp,]
}
z_temp = do.call(rbind, z_old)
mu_dat = exp(B_beta_mu_dat + B_beta_mu_u_dat)
# (5) Update the transition probabilities
pi_S_0 = rdirichlet(rep(alpha_S_old / Z_max, Z_max) + table_int(z_temp[idx_succ,1], Z_max))
pi_F_0 = rdirichlet(rep(alpha_F_old / Z_max, Z_max) + table_int(z_temp[-idx_succ,1], Z_max))
tr_count_S = count_assign(z_temp[idx_succ,], Z_max)
tr_count_F = count_assign(z_temp[-idx_succ,], Z_max)
#' @param tau vector of size n containing the response times
#' @param mu matrix of size (n x d1) containing the drift parameters
#' corresponding to the n response times for each possible d1 decision
#' @param b matrix of size (n x d1) containing the boundary parameters
#' corresponding to the n response times for each possible d1 decision
#' @param delta vector of size n containing the offset parameters corresponding
#' to the n response times
#' @param cens vector of size n containing censoring indicators (1 censored, 0
#' not censored) corresponding to the n response times
#' @param D (n x 2) matrix whose first column has the n input stimuli, and whose second column has the n decision categories
log_likelihood_ind <- function(tau, mu, b, delta, cens, D) {
.Call(`_lddmm_log_likelihood_ind`, tau, mu, b, delta, cens, D)
}
#' @param mu matrix of size (n x d1) containing the drift parameters
#' corresponding to the n response times for each possible d1 decision
#' @param b matrix of size (n x d1) containing the boundary parameters
#' corresponding to the n response times for each possible d1 decision
#' @param delta vector of size n containing the offset parameters corresponding
#' to the n response times
#' @param cens vector of size n containing censoring indicators (1 censored, 0
#' not censored) corresponding to the n response times
#' @param D (n x 2) matrix whose first column has the n input stimuli, and whose second column has the n decision categories
#' @param log should the results be returned on the log scale?
log_likelihood <- function(tau, mu, b, delta, cens, D, log) {
.Call(`_lddmm_log_likelihood`, tau, mu, b, delta, cens, D, log)
}
table_int <- function(x, K) {
.Call(`_lddmm_table_int`, x, K)
}
sum_rows_log <- function(Q) {
.Call(`_lddmm_sum_rows_log`, Q)
}
normalise_log <- function(x) {
.Call(`_lddmm_normalise_log`, x)
}
rdirichlet <- function(deltas) {
.Call(`_lddmm_rdirichlet`, deltas)
}
count_assign <- function(z, M) {
.Call(`_lddmm_count_assign`, z, M)
}
sample_reff_mu <- function(tau, D, cens, beta_u_old, delta_dat, b_dat, B_beta_dat, mu_dat_old, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u) {
.Call(`_lddmm_sample_reff_mu`, tau, D, cens, beta_u_old, delta_dat, b_dat, B_beta_dat, mu_dat_old, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u)
}
sample_reff_b <- function(tau, D, cens, beta_u_old, delta_dat, B_beta_dat, b_dat_old, mu_dat, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u) {
.Call(`_lddmm_sample_reff_b`, tau, D, cens, beta_u_old, delta_dat, B_beta_dat, b_dat_old, mu_dat, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u)
}
source("~/Desktop/Temp - 2021+ - Decision Making vs Working Memory/Codes/LDDMM-0.4/R/RcppExports.R", echo=TRUE)
fit1 <- LDDMM(data = data,
hypers = hypers,
boundaries = "fixed-constant",
Niter = Niter,
burnin = burnin,
thin = thin)
fixed
2*70
2*70*4
5*70
5*70*4
0.8*70
0.8*70*9
70*35
70*34
1400+504+560
1300/4
1300/20
1300/(4*70)
1400+504+560
56+350+140
readDta_reg
# Load relevant libraries, functions and data ----------------------------------
rm(list=ls())
# Set the working directory to the current folder
# Code to set the working directory to the current folder from RStudio
library(rstudioapi) # version 0.15
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr)
library(tidyr)
library(mvtnorm)
library(invgamma)
library(splines)
library(ggplot2)
theme_set(theme_bw(base_size = 14))
library(reshape2)
source("SEP_fcts.R")
## global variables for the dta
out = readDta_reg(file="Data-and-Results/data_protein.RData")
out
str(out)
# Load relevant libraries, functions and data ----------------------------------
rm(list=ls())
# Set the working directory to the current folder
# Code to set the working directory to the current folder from RStudio
library(rstudioapi) # version 0.15
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr)
library(tidyr)
library(mvtnorm)
library(invgamma)
library(splines)
library(ggplot2)
theme_set(theme_bw(base_size = 14))
library(reshape2)
source("SEP_fcts.R")
## global variables for the dta
out = readDta_reg(file="Data-and-Results/data_protein.RData")
## make global vars for the data
X = out$X
y = out$y
str(X)
summary(X)
X
summary(X)
str(X)
plt_reg_ggplot
summary(y)
str(y)
sum(is.na(3583))
sum(is.na(y))
sum(is.na(y))/(32*3583)
X
PL
PL
out
hist(y)
hist(out$y)
