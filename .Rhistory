}
else { # data at t = {k-1,k} influence the kth coefficient
idx_time = (k-1):k
}
for (h in 1:Z_max){ # loop over possible latent values
# tuples (combinations of covariates) that are clustered together via
# the latent h
idx_cov = matrix(idx_xy[which(z_temp[,k] == h),], length(which(z_temp[,k] == h)), p)
X_1k = unique(idx_cov[,1]) # all possible values of x in this cluster
X_2k = unique(idx_cov[,2]) # all possible values of y in this cluster
if (length(X_1k) > 0){ # h \in \mathcal{Z}_{j,k}: posterior update
# Pick data with covariate levels of x clustered in group h and
# at the correct locations
idx_i = which( (D[,1] %in% X_1k) & (time %in% idx_time) ) # AS: what about X_2k?
tau_temp = tau[idx_i]
cens_temp = cens[idx_i]
D_temp = D[which((D[,1] %in% X_1k) & (time %in% idx_time)),] # AS: what about X_2k?
# Normal proposal distribution centered on the current value
if (k == 1){
beta_mu_star_prop[,h] = beta_mu_star_old[,h]
beta_mu_star_prop[k,h] = rnorm(1, beta_mu_star_old[k,h], sd_MH_beta_mu)
}
# Normal proposal from the prior
else if (k == J){
beta_pre1 = beta_mu_star_old[k-1, unique(z_temp[which(z_temp[,k] == h),k-1])]
beta_mu_star_prop[,h] = beta_mu_star_old[,h]
beta_mu_star_prop[k,h] = rnorm(1, beta_pre1, sqrt(sigma2_1_mu_old))
}
# Normal proposal from the prior
else {
beta_pre1 = beta_mu_star_old[k-1, unique(z_temp[which(z_temp[,k] == h),k-1])]
beta_mu_star_prop[,h] = beta_mu_star_old[,h]
beta_mu_star_prop[k,h] = rnorm(1, beta_pre1, sqrt(sigma2_1_mu_old))
}
B_beta_mu_prop_dat = B_beta_mu_dat[idx_i,]
# Modify the proposed values in the corresponding positions
for (hh in 1:nrow(idx_cov)){
B_beta_mu_prop_dat[which(D_temp[,1] == idx_cov[hh,1]),idx_cov[hh,2]] =
B[idx_i[which(D_temp[,1] == idx_cov[hh,1])],] %*% beta_mu_star_prop[,h]
}
# This is the proposed value for \b_{x,y}^{(i)}(t), \mu_{x,y}^{(i)}(t)
mu_prop_dat = exp(B_beta_mu_prop_dat + B_beta_mu_u_dat[idx_i,])
if (k == 1){
beta_post1 = beta_mu_star_old[k+1, unique(z_temp[which(z_temp[,k] == h),k+1])]
logpost_prop = log_likelihood(tau_temp, mu_prop_dat,
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_prop[k,h] - beta_post1)^2)
logpost_old = log_likelihood(tau_temp, mu_dat[idx_i,],
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_old[k,h] - beta_post1)^2)
}
else if (k == J){
logpost_prop = log_likelihood(tau_temp, mu_prop_dat,
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE)
logpost_old = log_likelihood(tau_temp, mu_dat[idx_i,],
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE)
}
else {
beta_post1 = beta_mu_star_old[k+1, unique(z_temp[which(z_temp[,k] == h),k+1])]
logpost_prop = log_likelihood(tau_temp, mu_prop_dat,
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_prop[k,h] - beta_post1)^2)
logpost_old = log_likelihood(tau_temp, mu_dat[idx_i,],
b_dat[idx_i,], delta_dat[idx_i],
cens_temp, D_temp, TRUE) -
0.5/sigma2_1_mu_old * sum((beta_mu_star_old[k,h] - beta_post1)^2)
}
alpha_acc = min(0, logpost_prop - logpost_old)
l_u = log(runif(1))
if (l_u < alpha_acc){
beta_mu_star_old[k,h] = beta_mu_star_prop[k,h]
B_beta_mu_dat[idx_i,] = B_beta_mu_prop_dat
mu_dat[idx_i,] = mu_prop_dat
}
}
else { # h \notin \mathcal{Z}_{1,k}: prior sampling
beta_mu_star_old[k,h] = runif(1, low_bound_mu, upp_bound_mu)
}
}
}
# 2(a) Update the constant boundary parameters
tau_temp = tau
cens_temp = cens
D_temp = D
for (d2 in 1:d_j[2]){
b_prop = rnorm(1, b_old[d2], sd_MH_beta_b[d2])
# Modify the proposed values in the corresponding positions
B_beta_b_prop_dat = B_beta_b_dat
B_beta_b_prop_dat[,d2] = b_prop
# This is the proposed value for b
b_prop_dat = exp(B_beta_b_prop_dat)
logpost_prop = log_likelihood(tau_temp, mu_dat,
b_prop_dat, delta_dat,
cens_temp, D_temp, TRUE)
logpost_old = log_likelihood(tau_temp, mu_dat,
b_dat, delta_dat,
cens_temp, D_temp, TRUE)
alpha_acc = min(0, logpost_prop - logpost_old)
l_u = log(runif(1))
if (l_u < alpha_acc){
b_old[d2] = b_prop
# b_dat = b_prop_dat
B_beta_b_dat = B_beta_b_prop_dat
b_dat = b_prop_dat
acc_b[d2] = acc_b[d2] + 1
}
}
# (3) Update the cluster assignments
for (x_temp in 1:d_j[1]){ # loop over possible latent values # AS: why only 1
beta_mess = array(-Inf, dim = c(J, dim_HB))
beta_mess[J,] = 1/dim_HB
v_old[,J] = H_ball_unif(z_old[[x_temp]][,J], S = Z_max, r = r_HB)
z_prop[[J]] = H_ball(v_old[,J], S = Z_max, r = r_HB)
for (k in (J - 1):1){
idx_i = which( (time == k) & (D[,1] == x_temp) )
tau_temp = tau[idx_i]
cens_temp = cens[idx_i]
D_temp = D[idx_i,]
# (i) Sample the auxiliary variables
v_temp = H_ball(z_old[[x_temp]][,k], S = Z_max, r = r_HB)
probs = rep(-Inf, dim_HB)
for (h in 1:dim_HB){
B_beta_mu_prop_dat = B_beta_mu_dat[idx_i,]
B_beta_mu_prop_dat = 0.5 * (beta_mu_star_old[k,v_temp[,h]] +
beta_mu_star_old[k+1,z_old[[x_temp]][,k+1]])
mu_dat_prop = exp(t(B_beta_mu_prop_dat + t(B_beta_mu_u_dat[idx_i,])))
probs[h] = g_HB(log_likelihood(tau_temp, mu_dat_prop, b_dat[idx_i,],
delta_dat[idx_i], cens_temp, D_temp, TRUE))
}
probs = as.numeric(normalise_log(probs))
v_old[,k] = v_temp[,sample(1:dim_HB, 1, prob = probs)]
z_prop[[k]] = H_ball(v_old[,k], S = Z_max, r = r_HB)
# (ii) Pass messages backwards only in the restricted state space given
#      by the slice
z_kp1_temp = which(beta_mess[k+1,] > 0)
prob_mat = array(-Inf, dim = c(dim_HB, dim_HB))
for (h1 in z_kp1_temp){
for (h2 in 1:dim_HB){
B_beta_mu_prop_dat = B_beta_mu_dat[idx_i,]
B_beta_mu_prop_dat_1 = 0.5 * (beta_mu_star_old[k,z_prop[[k]][,h2]] +
beta_mu_star_old[k+1,z_prop[[k+1]][,h1]])
B_beta_mu_prop_dat_2 = 0.5 * (beta_mu_star_old[k,v_old[,k]] +
beta_mu_star_old[k+1,z_prop[[k+1]][,h1]])
mu_dat_prop_1 = exp(t(B_beta_mu_prop_dat_1 + t(B_beta_mu_u_dat[idx_i,])))
mu_dat_prop_2 = exp(t(B_beta_mu_prop_dat_2 + t(B_beta_mu_u_dat[idx_i,])))
prob_mat[h2,h1] = log(beta_mess[k+1,h1]) -
0.5 / sigma2_1_mu_old * sum((beta_mu_star_old[k,z_prop[[k]][,h2]] -
beta_mu_star_old[k+1,z_prop[[k+1]][,h1]])^2) +
log_likelihood(tau_temp, mu_dat_prop_1, b_dat[idx_i,],
delta_dat[idx_i], cens_temp, D_temp, TRUE) +
g_HB(log_likelihood(tau_temp, mu_dat_prop_2, b_dat[idx_i,],
delta_dat[idx_i], cens_temp, D_temp, TRUE)) +
sum(log(Q_F_old[cbind(z_prop[[k]][-x_temp,h2],z_prop[[k+1]][-x_temp,h1])])) +
log(Q_S_old[z_prop[[k]][x_temp,h2],z_prop[[k+1]][x_temp,h1]])
}
}
if ( sum(is.infinite(sum_rows_log(prob_mat))) == dim_HB){
beta_mess[k,] = 1/dim_HB
}
else{
beta_mess[k,] = as.numeric(sum_rows_log(prob_mat))
beta_mess[k,] = as.numeric(normalise_log(beta_mess[k,]))
}
}
# (iii) Sample states forward (only on allowed states)
idx_fail = (1:d_j[2])[-x_temp]
# Sample z_1
prob_vec = log(beta_mess[1,]) + log(pi_S_0[z_prop[[1]][x_temp,]]) +
colSums(matrix(log(pi_F_0[z_prop[[1]][-x_temp,]]), d_j[2] - 1, dim_HB))
prob_vec = as.numeric(normalise_log(prob_vec))
idx_samp = sample(1:dim_HB, 1, FALSE, prob_vec)
z_old[[x_temp]][,1] = z_prop[[1]][,idx_samp]
# Sample z_k
for (k in 2:J){
idx_km1 = which( (time == k - 1) & (D[,1] == x_temp) )
tau_temp = tau[idx_km1]
cens_temp = cens[idx_km1]
D_temp = D[idx_km1,]
prob_vec = log(beta_mess[k,]) +
log(Q_S_old[cbind(z_old[[x_temp]][x_temp,k-1], z_prop[[k]][x_temp,])])
for (kkk in idx_fail){
prob_vec = prob_vec + log(Q_F_old[cbind(z_old[[x_temp]][kkk,k-1], z_prop[[k]][kkk,])])
}
for (z_k_temp in which(is.finite(prob_vec))){
B_beta_mu_prop_dat = B_beta_mu_dat[idx_km1,]
B_beta_mu_prop_dat_1 = 0.5 * (beta_mu_star_old[k-1,z_old[[x_temp]][,k-1]] +
beta_mu_star_old[k,z_prop[[k]][,z_k_temp]])
B_beta_mu_prop_dat_2 = 0.5 * (beta_mu_star_old[k-1,v_old[,k-1]] +
beta_mu_star_old[k,z_prop[[k]][,z_k_temp]])
mu_dat_prop_1 = exp(t(B_beta_mu_prop_dat_1 + t(B_beta_mu_u_dat[idx_km1,])))
mu_dat_prop_2 = exp(t(B_beta_mu_prop_dat_2 + t(B_beta_mu_u_dat[idx_km1,])))
prob_vec[z_k_temp] = prob_vec[z_k_temp] -
0.5 / sigma2_1_mu_old * sum((beta_mu_star_old[k-1,z_old[[x_temp]][,k-1]] -
beta_mu_star_old[k,z_prop[[k]][,z_k_temp]])^2) +
log_likelihood(tau_temp, mu_dat_prop_1, b_dat[idx_km1,],
delta_dat[idx_km1], cens_temp, D_temp, TRUE) +
g_HB(log_likelihood(tau_temp, mu_dat_prop_2, b_dat[idx_km1,],
delta_dat[idx_km1], cens_temp, D_temp, TRUE))
}
prob_vec = as.numeric(normalise_log(prob_vec))
idx_samp = sample(1:dim_HB, 1, FALSE, prob_vec)
z_old[[x_temp]][,k] = z_prop[[k]][,idx_samp]
}
# (4) Assign the cluster specific curves f_{\mu}
for (y_temp in 1:d_j[2]){
beta_mu_old[,x_temp,y_temp] = beta_mu_star_old[cbind(1:J, z_old[[x_temp]][y_temp,])]
}
B_beta_mu_dat[(D[,1] == x_temp),] = B[D[,1] == x_temp,] %*% beta_mu_old[,x_temp,]
}
z_temp = do.call(rbind, z_old)
mu_dat = exp(B_beta_mu_dat + B_beta_mu_u_dat)
# (5) Update the transition probabilities
pi_S_0 = rdirichlet(rep(alpha_S_old / Z_max, Z_max) + table_int(z_temp[idx_succ,1], Z_max))
pi_F_0 = rdirichlet(rep(alpha_F_old / Z_max, Z_max) + table_int(z_temp[-idx_succ,1], Z_max))
tr_count_S = count_assign(z_temp[idx_succ,], Z_max)
tr_count_F = count_assign(z_temp[-idx_succ,], Z_max)
#' @param tau vector of size n containing the response times
#' @param mu matrix of size (n x d1) containing the drift parameters
#' corresponding to the n response times for each possible d1 decision
#' @param b matrix of size (n x d1) containing the boundary parameters
#' corresponding to the n response times for each possible d1 decision
#' @param delta vector of size n containing the offset parameters corresponding
#' to the n response times
#' @param cens vector of size n containing censoring indicators (1 censored, 0
#' not censored) corresponding to the n response times
#' @param D (n x 2) matrix whose first column has the n input stimuli, and whose second column has the n decision categories
log_likelihood_ind <- function(tau, mu, b, delta, cens, D) {
.Call(`_lddmm_log_likelihood_ind`, tau, mu, b, delta, cens, D)
}
#' @param mu matrix of size (n x d1) containing the drift parameters
#' corresponding to the n response times for each possible d1 decision
#' @param b matrix of size (n x d1) containing the boundary parameters
#' corresponding to the n response times for each possible d1 decision
#' @param delta vector of size n containing the offset parameters corresponding
#' to the n response times
#' @param cens vector of size n containing censoring indicators (1 censored, 0
#' not censored) corresponding to the n response times
#' @param D (n x 2) matrix whose first column has the n input stimuli, and whose second column has the n decision categories
#' @param log should the results be returned on the log scale?
log_likelihood <- function(tau, mu, b, delta, cens, D, log) {
.Call(`_lddmm_log_likelihood`, tau, mu, b, delta, cens, D, log)
}
table_int <- function(x, K) {
.Call(`_lddmm_table_int`, x, K)
}
sum_rows_log <- function(Q) {
.Call(`_lddmm_sum_rows_log`, Q)
}
normalise_log <- function(x) {
.Call(`_lddmm_normalise_log`, x)
}
rdirichlet <- function(deltas) {
.Call(`_lddmm_rdirichlet`, deltas)
}
count_assign <- function(z, M) {
.Call(`_lddmm_count_assign`, z, M)
}
sample_reff_mu <- function(tau, D, cens, beta_u_old, delta_dat, b_dat, B_beta_dat, mu_dat_old, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u) {
.Call(`_lddmm_sample_reff_mu`, tau, D, cens, beta_u_old, delta_dat, b_dat, B_beta_dat, mu_dat_old, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u)
}
sample_reff_b <- function(tau, D, cens, beta_u_old, delta_dat, B_beta_dat, b_dat_old, mu_dat, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u) {
.Call(`_lddmm_sample_reff_b`, tau, D, cens, beta_u_old, delta_dat, B_beta_dat, b_dat_old, mu_dat, B, P, ind, time, sigma2_us, sigma2_ua, sd_beta_u, acc_beta_u)
}
source("~/Desktop/Temp - 2021+ - Decision Making vs Working Memory/Codes/LDDMM-0.4/R/RcppExports.R", echo=TRUE)
fit1 <- LDDMM(data = data,
hypers = hypers,
boundaries = "fixed-constant",
Niter = Niter,
burnin = burnin,
thin = thin)
fixed
2*70
2*70*4
5*70
5*70*4
0.8*70
0.8*70*9
70*35
70*34
1400+504+560
1300/4
1300/20
1300/(4*70)
1400+504+560
56+350+140
heatmap(y[order(mik_star[1,]),s_star==1],Colv = NA, Rowv = NA)
heatmap(y[order(mik_star[2,]),s_star==2],Colv = NA, Rowv = NA)
data_country=pivot_wider(data[,c(1:2,6)],names_from = Sample,values_from = nationality)
pivot_wider
# Load relevant libraries, functions and data ----------------------------------
rm(list=ls())
# Set the working directory to the current folder
# Code to set the working directory to the current folder from RStudio
library(rstudioapi) # version 0.15
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(DirichletReg)
library(invgamma)
library(tidyr)
library(dplyr)
library(truncnorm)
library(scales)
library(viridisLite)
library(ggplot2)
theme_set(theme_bw(base_size = 14))
library(RColorBrewer)
library(salso)
###################################################N
## create global variables for data & field dimensions
# Read and clean the data
###################################################N
## read in data
read.dta  <- function()
{##read-in and clean data
Dietswap_dataset <- readRDS("Data-and-Results/Dietswap_dataset.RDS")
data = Dietswap_dataset[Dietswap_dataset$timepoint==1,]
data2=pivot_wider(data[,1:3],names_from = Sample,values_from = Abundance)
na_in_rows=sapply(1:dim(data2)[1],function(i) sum(data2[i,]==0))
data3=data.frame(data2[na_in_rows!=38,]) #remove all na rows
Y0=data3[,2:39]                ## separate OTU counts
rownames(Y0)=data3[,1]         ## and OTU names as row names..
n <- dim(Y0)[2]                ## n patients
drop = apply(Y0==0,1,sum)>n/4  ## drop all OTUS with more than n/4 0's
Y  <- Y0[!drop,]               ## dropping 20 of the originally 119 OTU's
## now total # 0's is only 5.
gammas=colMeans(Y)
Y=sapply(1:n,function(i) as.matrix(Y)[,i]/gammas[i])
y=log(Y+0.05)                  ## log transform (we only have 5 0's)
return(list(y=y,Y=Y)) ## return both - the log transf "y" and raw "Y"
}
## create global variables for data & fied dimensions
out <- read.dta() ## maybe as.matrix(..)?? See if needed..
y <-   out$y    ## log transf
Y  <-  out$Y    ## abs scale
N <- dim(y)[1]*dim(y)[2]  # global var's
n <- dim(y)[2] # # patients
B <- dim(y)[1] # # OTU's
##### max size of the subject (K) and OTU (L) partition
K = 10
L = 15
## hyperprior pars
alpha = 1      ## for GEM prior on pi
beta  = 1      ## GEM prior on w[k], k = 1, ..., K
## hyperpars (use "empirical Bayes" i.e., match sample moments)
mu0  = mean(as.matrix(y))                  ## mu_l ~ N(mu0, sig0)
sig0 = 3  ## SD!
a0 = 3                                    ## sig2_l ~ Ga(a0/2, b0/2)
b0 = var(c(y))/4
source("SEP_fcts.R")
############ exploratory plots ###########
Exploratory = FALSE
if(Exploratory){
Dietswap_dataset <- readRDS("Data-and-Results/Dietswap_dataset.RDS")
data = Dietswap_dataset[Dietswap_dataset$timepoint==1,]
data2=pivot_wider(data[,1:3],names_from = Sample,values_from = Abundance)
na_in_rows=sapply(1:dim(data2)[1],function(i) sum(data2[i,]==0))
data3=data.frame(data2[na_in_rows!=38,]) #remove all na rows
y=data3[,2:39]
gammas=colMeans(y)
y=sapply(1:n,function(i) as.matrix(y)[,i]/gammas[i])
y1=y[order(rowSums(y),decreasing = T),]
data_country=pivot_wider(data[,c(1:2,6)], names_from = Sample,
values_from = nationality)
data_country=data_country[1,2:dim(data_country)[2]]
data_country=as.matrix(data_country)[1,]
names(data_country)=NULL
data_label=data_country
data_label[data_country=='AFR']=1
data_label[data_country=='AAM']=2
data_label2 = data_country
data_label2[data_country=='AFR'] = 'AF'
data_label2[data_country=='AAM'] = 'AA'
y_af=as.matrix(y1[,data_country=='AFR'])
y_am=as.matrix(y1[,data_country=='AAM'])
pdf_af=rowSums(y_af)/sum(rowSums(y_af))
pdf_am=rowSums(y_am)/sum(rowSums(y_am))
pdf_all=rowSums(y1)/sum(rowSums(y1))
P = ggplot()+
geom_line(aes(0:119,c(0,cumsum(pdf_af)),col='AF'))+
geom_line(aes(0:119,c(0,cumsum(pdf_am)),col='AA'))+
geom_line(aes(0:119,c(0,cumsum(pdf_all)),col='ALL'))+
labs(x='OTU', y="Cumulative Relative Frequencies")+
theme(legend.position = 'right')+  labs(color=NULL)
ggsave(plot=P, file ="Image/Emp_CDF_OTU.pdf",
width=16, height=12, units = 'cm')
dfplot_af=cbind(1:119,rowMeans(y_af)) %>% data.frame()
colnames(dfplot_af) =  c("OTU","Abundace")
P1 = ggplot(dfplot_af,aes(OTU,Abundace))+
geom_bar(stat='identity')+
labs(x='OTU (AF)',y='Scaled Abundance')
ggsave(plot=P1, file ="Image/normedcount-hist-af.pdf",
width=5, height=3.5, units = 'in')
dfplot_am=cbind(1:119,rowMeans(y_am)) %>% data.frame()
colnames(dfplot_am) =  c("OTU","Abundace")
P2 = ggplot(dfplot_am,aes(OTU,Abundace))+
geom_bar(stat='identity')+
labs(x='OTU (AA)',y='Scaled Abundance')
ggsave(plot=P2, file ="Image/normedcount-hist-am.pdf",
width=5, height=3.5, units = 'in')
#######plot dendograms based on hierarchical clustering
colvar <- function(y){
return(sapply(1:dim(y)[2],function(j) var(y[,j])))
}
rowvar <- function(y){
return(sapply(1:dim(y)[1],function(i) var(y[i,])))
}
rvy=rowvar(y)
yhc=y[order(rvy,decreasing = T)<=10,]
rownames(yhc)=rownames(y)[order(rvy,decreasing = T)<=10]
colnames(yhc)=data_label
d <- dist(yhc, method = "euclidean")
hc2 <- hclust(d, method = "complete" )
plot(hc2,cex=0.5,main='')
d <- dist(t(yhc), method = "euclidean")
hc2 <- hclust(d, method = "complete" )
plot(hc2,cex=0.5,main='')
dend_expr <- as.dendrogram(hc2)
tree_labels <- dendro_data(dend_expr, type = "rectangle")
tree_labels$labels <- cbind(tree_labels$labels, Subject = as.factor(data_label2))
P = ggplot() +
geom_segment(data =segment(tree_labels), aes(x=x, y=y, xend=xend, yend=yend))+
geom_segment(data = tree_labels$segments %>%
filter(yend == 0) %>%
left_join(tree_labels$labels, by = "x"),
aes(x=x, y=y.x, xend=xend, yend=yend, color = Subject)) +
geom_text(data = label(tree_labels),
aes(x=x, y=y, label=label, hjust=-1), size=2) +
coord_flip() +
scale_y_reverse(expand=c(0.2, 0)) +
scale_colour_brewer(palette = "Dark2") +
theme_dendro()+
theme(legend.title=element_blank())
ggsave(plot=P, file ="Image/otu-subject-hc.pdf",
width=5, height=3.5, units = 'in')
}
# Plot Results
# Run analysis, save results and produce some plots
Run_MCMC = FALSE
if(Run_MCMC){
ex(niter  = 1e4, #iteration MCMC
niter0 = 1e3, # estimating Sj after niter0 iterations and stop updating Sj
niter1 = 2e3 # just for plot
)
}
# Load output
pi    <- read.myfile("pi.txt")
Sj    <- read.myfile("Sj.txt")
w     <- read.myfile("w.txt", K, L) # w[k, iter, l]
mki   <- read.myfile("mki.txt", K, K*B)
mu    <- read.myfile("mu.txt")
sig2  <- read.myfile("sig.txt")
# nk=sapply(1:K,function(k) sum(Sj==k))
nk    <- read.myfile("nk.txt")
# nl=sapply(1:L,function(l) sum(c(mki)==l))
nl    <- read.myfile("nl.txt")
ll    <- read.myfile("logl.txt")
nkp   <- sum(nk>0)
nlp   <- sum(nl>0)
# summ  <- c(iter, nkp, nlp, nk, nl, ll, pmki)
summ  <- read.myfile("iter.txt")
niter <- dim(w)[2]
library(salso)
point_SJ = salso::salso(Sj, nRuns = 100, maxZealousAttempts = 100, loss=VI())
length(point_SJ)
# Gio's plot
library(reshape2)
#ggplot needs a dataframe
data <- as.data.frame(t(y))
#plot
# Distributional clusters by OTU
Dietswap_dataset <- readRDS("Data-and-Results/Dietswap_dataset.RDS")
data = Dietswap_dataset[Dietswap_dataset$timepoint==1,]
data2=pivot_wider(data[,1:3],names_from = Sample,values_from = Abundance)
na_in_rows=sapply(1:dim(data2)[1],function(i) sum(data2[i,]==0))
data3=data.frame(data2[na_in_rows!=38,]) #remove all na rows
data=data3[,2:39]
# This plot is made adapting the code in Fradenti Github rep CommonAtomModel
cluster_per_obs <- rep(point_SJ, rep(nrow(data), ncol(data)))
LL <- apply(data,2,function(x) cumsum(sort((x),decreasing = T)) /sum((x)) )
dim(LL)
LL2 <- reshape2::melt(LL)
LL2 <- cbind(LL2,cluster_per_obs)
LL2 <- as_tibble(LL2) %>% mutate(cluster_per_obs=paste0("",cluster_per_obs))
P = ggplot(LL2)+
geom_hline(yintercept = c(0,1),lty=3)+
geom_line(aes(x=Var1,y=value,group=Var2,col=as.factor(cluster_per_obs)),
lwd=.5, alpha=.2)+
geom_point(aes(x=Var1,y=value,group=Var2,col=as.factor(cluster_per_obs)),
alpha=.9)+
theme_bw()+scale_color_manual("Subject\nCluster",
values = c(2,4,1,6,5)) + ylim(0, 1)+
xlab("Taxa sorted by count")+ylab("Cumulative Relative Frequncy")
ggsave(plot=P, file="Image/Sjclustergg2.pdf", height = 2.5, width = 4)
data_country=pivot_wider(data[,c(1:2,6)],names_from = Sample,values_from = nationality)
data_country=data_country[1,2:dim(data_country)[2]]
y[order(mik_star[1,]), point_SJ==1]
data_country=pivot_wider(data[,c(1:2,6)],names_from = Sample,values_from = nationality)
data
str(data)
heatmap(data[order(mki[1,]),s_star==1], Colv = NA, Rowv = NA)
heatmap(data[order(mki[2,]),s_star==2], Colv = NA, Rowv = NA)
heatmap(data[order(mki[3,]),s_star==3], Colv = NA, Rowv = NA)
heatmap(data[order(mki[1,]), point_SJ==1], Colv = NA, Rowv = NA)
heatmap(data[order(mki[2,]), point_SJ==2], Colv = NA, Rowv = NA)
heatmap(data[order(mki[3,]), point_SJ==3], Colv = NA, Rowv = NA)
str(mki)
mki[1,]
mki[2,]
